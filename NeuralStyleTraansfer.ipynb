{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Style Network**"
      ],
      "metadata": {
        "id": "csxrcEMdXMd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "aETznOCGXvVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "oV1UXW-CXriN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The Model"
      ],
      "metadata": {
        "id": "r5FmZKqBYXvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the pretrained VGG19 model here. It has 3 components:\n",
        "\n",
        "1.   features, that hold all the convolutional, max pool ans ReLu layers\n",
        "2.   avgpool, that holds average pool layer\n",
        "3.   classifier, that holds dense layers\n",
        "\n",
        "We are going to use onyl convolutional neural network for simplicity in training. Hence are imcludong onyl features."
      ],
      "metadata": {
        "id": "uqWA1wutYjMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "model=models.vgg19(pretrained=True).features\n",
        "device=torch.device( \"cuda\" if (torch.cuda.is_available()) else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYdYr9YGX-rD",
        "outputId": "bdcc1e87-360f-4baf-97f4-18618bcfa5ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Preprocessing"
      ],
      "metadata": {
        "id": "oxFe-DN9a1c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using torch.tranform to\n",
        "\n",
        "1.   resize all the images to 512 x 512\n",
        "2.   to convert images into tensor\n",
        "\n",
        "Then we will load the content and style images, and we will use a clone of content image and modify it to obtain the results. We will have to allow the clone image to be modified by gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "OuHVxwP4b_KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(path):\n",
        "    image = Image.open(path)\n",
        "    loader=transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
        "    image=loader(image).unsqueeze(0)\n",
        "    return image.to(device,torch.float)\n",
        "\n",
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
        "\n",
        "original_image=image_loader(content_path)\n",
        "style_image=image_loader(style_path)\n",
        "generated_image=original_image.clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "5Wr77b1zasyI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Representation"
      ],
      "metadata": {
        "id": "F2Ocwlh5f1__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defininf a class that will provide the feature representations of the intermediate layers. In this class, we will initialize a model by eliminating the unused layers of the vgg19 model and extract the activations or the feature representations of the ‘conv1_1’, ‘conv2_1’, ‘conv3_1’, ‘conv4_1’ and ‘conv5_1’ layers (index values [0, 5, 10, 19, 28]). Store these activations of 5 convolutional layers in an array and return the array."
      ],
      "metadata": {
        "id": "rMxOJ702g70K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG,self).__init__()\n",
        "        self.req_features= ['0','5','10','19','28']\n",
        "        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
        "\n",
        "    #x holds the input tensor(image) that will be feeded to each layer\n",
        "    def forward(self,x):\n",
        "        features=[]\n",
        "        for layer_num,layer in enumerate(self.model):\n",
        "            #activation of the layer will stored in x\n",
        "            x=layer(x)\n",
        "            #appending the activation of the selected layers and return the feature array\n",
        "            if (str(layer_num) in self.req_features):\n",
        "                features.append(x)\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "ElkeAQK7g8e5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Losses"
      ],
      "metadata": {
        "id": "B-wkqbGBhT1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have discussed about the losses in detail in documentaion.\n",
        "\n",
        "In brief, we have content and style losses, and the total loss is linear sum fo these two losses."
      ],
      "metadata": {
        "id": "BZmx-Se-haUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Content loss\n",
        "def calc_content_loss(gen_feat,orig_feat):\n",
        "    #Calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
        "    content_l=torch.mean((gen_feat-orig_feat)**2)\n",
        "    return content_l\n",
        "\n",
        "# Style loss\n",
        "def calc_style_loss(gen,style):\n",
        "    #Calculating the gram matrix for the style and the generated image\n",
        "    batch_size,channel,height,width=gen.shape\n",
        "\n",
        "    G=torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n",
        "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
        "\n",
        "    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n",
        "    style_l=torch.mean((G-A)**2)\n",
        "    return style_l\n",
        "\n",
        "# Total loss\n",
        "def calculate_loss(gen_features, orig_feautes, style_featues):\n",
        "    style_loss=content_loss=0\n",
        "    for gen,cont,style in zip(gen_features,orig_feautes,style_featues):\n",
        "        #extracting the dimensions from the generated image\n",
        "        content_loss+=calc_content_loss(gen,cont)\n",
        "        style_loss+=calc_style_loss(gen,style)\n",
        "\n",
        "    #calculating the total loss of e th epoch\n",
        "    total_loss=alpha*content_loss + beta*style_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "wNb20-lOhuzu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "Pftj0z0Mi13w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up hyperparameters\n",
        "model=VGG().eval()\n",
        "\n",
        "epoch=2500\n",
        "lr=0.004\n",
        "alpha=8\n",
        "beta=70\n",
        "\n",
        "#Using adam optimizer, it will update the generated image not the model parameter\n",
        "optimizer=optim.Adam([generated_image],lr=lr)"
      ],
      "metadata": {
        "id": "OlLIoLqQilnq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range (epoch):\n",
        "    gen_features=model(generated_image)\n",
        "    orig_feautes=model(original_image)\n",
        "    style_featues=model(style_image)\n",
        "\n",
        "    total_loss=calculate_loss(gen_features, orig_feautes, style_featues)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if(e/100):\n",
        "        print(total_loss)\n",
        "\n",
        "        save_image(generated_image,\"gen.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGpdLfniNCbG",
        "outputId": "bc557729-047f-40e2-998e-792cbbf63993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.9024e+10, grad_fn=<AddBackward0>)\n",
            "tensor(2.6527e+10, grad_fn=<AddBackward0>)\n",
            "tensor(2.4293e+10, grad_fn=<AddBackward0>)\n",
            "tensor(2.2368e+10, grad_fn=<AddBackward0>)\n",
            "tensor(2.0615e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.8946e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.7394e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.5979e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.4698e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.3551e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.2535e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.1639e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.0847e+10, grad_fn=<AddBackward0>)\n",
            "tensor(1.0142e+10, grad_fn=<AddBackward0>)\n",
            "tensor(9.5112e+09, grad_fn=<AddBackward0>)\n",
            "tensor(8.9429e+09, grad_fn=<AddBackward0>)\n",
            "tensor(8.4276e+09, grad_fn=<AddBackward0>)\n",
            "tensor(7.9575e+09, grad_fn=<AddBackward0>)\n",
            "tensor(7.5277e+09, grad_fn=<AddBackward0>)\n",
            "tensor(7.1345e+09, grad_fn=<AddBackward0>)\n",
            "tensor(6.7754e+09, grad_fn=<AddBackward0>)\n",
            "tensor(6.4475e+09, grad_fn=<AddBackward0>)\n",
            "tensor(6.1466e+09, grad_fn=<AddBackward0>)\n",
            "tensor(5.8695e+09, grad_fn=<AddBackward0>)\n",
            "tensor(5.6136e+09, grad_fn=<AddBackward0>)\n",
            "tensor(5.3759e+09, grad_fn=<AddBackward0>)\n",
            "tensor(5.1539e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.9456e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.7494e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.5635e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.3869e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.2187e+09, grad_fn=<AddBackward0>)\n",
            "tensor(4.0582e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.9046e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.7578e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.6171e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.4823e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.3528e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.2286e+09, grad_fn=<AddBackward0>)\n",
            "tensor(3.1095e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.9954e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.8860e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.7812e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.6808e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.5846e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.4926e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.4045e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.3202e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.2396e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.1625e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.0888e+09, grad_fn=<AddBackward0>)\n",
            "tensor(2.0183e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.9509e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.8863e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.8245e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.7654e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.7088e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.6547e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.6029e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.5533e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.5059e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.4606e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.4172e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.3756e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.3359e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.2978e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.2613e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.2264e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.1929e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.1609e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.1302e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.1007e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.0725e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.0455e+09, grad_fn=<AddBackward0>)\n",
            "tensor(1.0195e+09, grad_fn=<AddBackward0>)\n",
            "tensor(9.9466e+08, grad_fn=<AddBackward0>)\n",
            "tensor(9.7082e+08, grad_fn=<AddBackward0>)\n",
            "tensor(9.4798e+08, grad_fn=<AddBackward0>)\n",
            "tensor(9.2607e+08, grad_fn=<AddBackward0>)\n",
            "tensor(9.0505e+08, grad_fn=<AddBackward0>)\n",
            "tensor(8.8489e+08, grad_fn=<AddBackward0>)\n",
            "tensor(8.6554e+08, grad_fn=<AddBackward0>)\n",
            "tensor(8.4695e+08, grad_fn=<AddBackward0>)\n",
            "tensor(8.2911e+08, grad_fn=<AddBackward0>)\n",
            "tensor(8.1197e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.9551e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.7968e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.6448e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.4986e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.3581e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.2229e+08, grad_fn=<AddBackward0>)\n",
            "tensor(7.0928e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.9676e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.8470e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.7309e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.6190e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.5112e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.4072e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.3070e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.2102e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.1169e+08, grad_fn=<AddBackward0>)\n",
            "tensor(6.0267e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.9396e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.8554e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.7740e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.6953e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.6191e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.5454e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.4741e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.4050e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.3381e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.2732e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.2102e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.1491e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.0898e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.0322e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.9763e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.9219e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.8690e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.8176e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.7675e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.7189e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.6715e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.6254e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.5804e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.5366e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.4939e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.4522e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.4115e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.3717e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.3329e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.2950e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.2579e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.2217e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.1862e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.1516e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.1176e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.0844e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.0519e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.0200e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.9888e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.9583e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.9283e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.8989e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.8701e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.8418e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.8141e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.7868e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.7601e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.7338e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.7080e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.6826e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.6577e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.6331e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.6090e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.5853e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.5620e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.5391e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.5165e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4942e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4723e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4508e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4295e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4086e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.3880e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.3677e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.3477e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.3280e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.3085e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.2894e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.2705e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.2518e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.2335e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.2154e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1975e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1799e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1625e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1453e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1284e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1117e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0952e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0789e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0628e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0469e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0313e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0158e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.0005e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9854e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9705e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9558e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9413e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9269e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.9127e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8987e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8848e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8711e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8575e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8441e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8309e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8178e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8048e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7920e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7793e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7667e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7543e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7421e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7299e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7179e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.7060e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6942e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6826e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6710e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6596e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6483e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6371e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6260e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6151e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6042e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5935e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5829e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5723e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5619e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5516e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5413e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5312e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5212e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5112e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.5014e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4916e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4820e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4724e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4629e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4535e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4442e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4350e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4259e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4168e+08, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}